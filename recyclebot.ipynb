{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioYgJTSpjcR9",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip /content/drive/MyDrive/red_data_no_webp.zip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def list_file_extensions(directory):\n",
        "    # Store unique file extensions\n",
        "    extensions = set()\n",
        "\n",
        "    # Walk through the directory\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            # Get the file extension and add to the set (ignoring empty extensions)\n",
        "            ext = os.path.splitext(file)[1]\n",
        "            if ext:  # Only add extensions that are not empty\n",
        "                extensions.add(ext.lower())\n",
        "\n",
        "    # Return sorted list of unique extensions\n",
        "    return sorted(extensions)\n",
        "\n",
        "# Example usage:\n",
        "directory_path = input(\"Enter the directory path: \")\n",
        "extensions = list_file_extensions('/content/red_data/img/')\n",
        "\n",
        "print(\"Unique file extensions in the directory:\")\n",
        "for ext in extensions:\n",
        "    print(ext)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X09AuXQvcOh",
        "outputId": "125bf1eb-2a66-4148-abf0-f4475f31f504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the directory path: fvgbnm,.\n",
            "Unique file extensions in the directory:\n",
            ".jpeg\n",
            ".jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "extracted_files = os.listdir('/content/red_data/img/')\n",
        "print(extracted_files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK7ARKrBNApM",
        "outputId": "7a84561e-91ab-4d95-af0c-bbb498867d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Metal', 'Plastic-Regular', 'Glass', 'Paperboard', 'Plastic-Polystyrene']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS-23JvPbQL-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Set image dimensions\n",
        "img_height = 240\n",
        "img_width = 240\n",
        "batch_size = 8\n",
        "\n",
        "# Path to your image directory\n",
        "str_path = '/content/red_data/img/'\n",
        "data_dir = pathlib.Path(str_path)\n",
        "\n",
        "# Load the datasets without applying cache/prefetch initially\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.15,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.15,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Now access class_names before any dataset transformation\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)  # You should be able to access class names here\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip('horizontal'),\n",
        "    layers.RandomRotation(0.2),\n",
        "    layers.RandomZoom(0.2),\n",
        "    layers.RandomContrast(0.2),\n",
        "])\n",
        "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "# Apply caching and prefetching after accessing class names\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
         "num_classes = 5\n",
        "for images, labels in train_ds.take(1):\n",
        "    print(\"Image shape:\", images.shape)\n",
        "# Define the base pre-trained model (ResNet50)\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "base_model.summary()\n",
        "# Freeze the base model (all layers initially frozen)\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create the custom model on top of the pre-trained model\n",
        "model = models.Sequential([\n",
        "    base_model,  # Pre-trained ResNet50 model\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_classes, activation='softmax')  # Output layer with softmax\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def custom_metric(y_true, y_pred):\n",
        "    return classification_report(y_true, y_pred, target_names=class_names)\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss'),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
        "\n",
        "]\n",
        "\n",
        "# Train the model with class weights\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=15,\n",
        "    callbacks=callbacks,\n",
        "    #class_weight=class_weights_dict\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# After training the new layers, unfreeze some layers of the base model to fine-tune\n",
        "base_model.trainable = True\n",
        "\n",
        "# Freeze the earlier layers of ResNet50 to prevent them from being modified too much during fine-tuning\n",
        "for layer in base_model.layers[:100]:  # You can experiment with which layers to unfreeze\n",
        "    layer.trainable = False\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def custom_metric(y_true, y_pred):\n",
        "    return classification_report(y_true, y_pred, target_names=class_names)\n",
        "\n",
        "# Re-compile the model for fine-tuning with a lower learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fine-tune the model\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=15,\n",
        "    callbacks=callbacks,\n",
        "    #class_weight=class_weights_dict\n",
        ")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save('recyclebot.keras')\n",
        "print(\"Transfer learning completed and model saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sytDDiWabXbG"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qymi9Gwo3mgf"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
